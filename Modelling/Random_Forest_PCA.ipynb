{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (400955, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tifffile</th>\n",
       "      <th>csvfile</th>\n",
       "      <th>Year</th>\n",
       "      <th>SAM</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>repnum</th>\n",
       "      <th>Loc</th>\n",
       "      <th>SITE</th>\n",
       "      <th>DOY</th>\n",
       "      <th>...</th>\n",
       "      <th>Sigma.Intensity</th>\n",
       "      <th>Roughness</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Image.File</th>\n",
       "      <th>Particle.ID</th>\n",
       "      <th>Image.Height</th>\n",
       "      <th>Image.Width</th>\n",
       "      <th>Image.X</th>\n",
       "      <th>Image.Y</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>18.4704</td>\n",
       "      <td>1.1331</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>1.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>16.2199</td>\n",
       "      <td>1.0927</td>\n",
       "      <td>0.7494</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>6.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1651</td>\n",
       "      <td>1.3904</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>10.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>26.3646</td>\n",
       "      <td>1.0824</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>12.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0291</td>\n",
       "      <td>1.0913</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>14.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tifffile  \\\n",
       "0  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "1  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "2  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "3  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "4  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "\n",
       "                                   csvfile  Year  SAM  Month  Day  repnum  \\\n",
       "0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "1  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "2  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "3  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "4  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "\n",
       "     Loc SITE  DOY  ...  Sigma.Intensity  Roughness  Transparency  \\\n",
       "0  FISHI  STA   97  ...          18.4704     1.1331        0.7957   \n",
       "1  FISHI  STA   97  ...          16.2199     1.0927        0.7494   \n",
       "2  FISHI  STA   97  ...          12.1651     1.3904        0.7751   \n",
       "3  FISHI  STA   97  ...          26.3646     1.0824        0.7866   \n",
       "4  FISHI  STA   97  ...          22.0291     1.0913        0.7755   \n",
       "\n",
       "                                Image.File  Particle.ID  Image.Height  \\\n",
       "0  04072021_Huron_6_2mm_Rep4_AD_000001.tif          1.0         295.0   \n",
       "1  04072021_Huron_6_2mm_Rep4_AD_000001.tif          6.0         263.0   \n",
       "2  04072021_Huron_6_2mm_Rep4_AD_000001.tif         10.0         166.0   \n",
       "3  04072021_Huron_6_2mm_Rep4_AD_000001.tif         12.0         253.0   \n",
       "4  04072021_Huron_6_2mm_Rep4_AD_000001.tif         14.0         173.0   \n",
       "\n",
       "   Image.Width  Image.X  Image.Y                                 Filename  \n",
       "0        276.0      0.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  \n",
       "1         69.0    532.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  \n",
       "2        106.0    862.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  \n",
       "3         78.0    993.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  \n",
       "4         79.0      0.0    297.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset\n",
    "data_path = \"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_All_Clean.csv\"  # Update path if necessary\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8416904640171591\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.78      0.82      0.80      1390\n",
      "      Bubbles       0.94      0.86      0.90       945\n",
      "   Calanoid_1       0.80      0.85      0.83     16075\n",
      "   Chironomid       0.78      0.56      0.65       303\n",
      "   Chydoridae       0.00      0.00      0.00        18\n",
      "   CopepodSpp       0.51      0.34      0.41      1725\n",
      "   CountGT500       0.69      0.33      0.44      2575\n",
      "      Cyclo_2       0.59      0.44      0.50      3248\n",
      "  Cyclopoid_1       0.77      0.87      0.81     16973\n",
      "      Daphnia       0.50      0.11      0.18        83\n",
      "       Floc_1       0.93      0.94      0.93     19836\n",
      "Herpacticoida       0.31      0.04      0.07       288\n",
      "     LargeZ-1       0.95      0.98      0.96     15751\n",
      "        other       0.48      0.22      0.30       981\n",
      "\n",
      "     accuracy                           0.84     80191\n",
      "    macro avg       0.64      0.52      0.56     80191\n",
      " weighted avg       0.83      0.84      0.83     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is your target column\n",
    "# And your selected features are as defined\n",
    "features = [\"WaterT\", \"AvgDepth\", \"PRECIP\", \"gdd2\", \"DOY\", \"YPerchDen\", \n",
    "            \"BurbotDen\", \"OtherFishDen\", \"distshore\", \"Area..ABD.\", \"Aspect.Ratio\", \n",
    "            \"Circularity\", \"Perimeter\", \"Diameter..ABD.\", \"Diameter..ESD.\"]\n",
    "\n",
    "# Extract features and target\n",
    "X = df[features]\n",
    "y = df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 10 components\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805339751343667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.72      0.81      0.76      1390\n",
      "      Bubbles       0.87      0.91      0.89       945\n",
      "   Calanoid_1       0.84      0.78      0.81     16075\n",
      "   Chironomid       0.47      0.75      0.58       303\n",
      "   Chydoridae       0.07      0.28      0.11        18\n",
      "   CopepodSpp       0.30      0.45      0.36      1725\n",
      "   CountGT500       0.41      0.48      0.44      2575\n",
      "      Cyclo_2       0.43      0.56      0.49      3248\n",
      "  Cyclopoid_1       0.81      0.72      0.76     16973\n",
      "      Daphnia       0.13      0.30      0.18        83\n",
      "       Floc_1       0.95      0.90      0.93     19836\n",
      "Herpacticoida       0.14      0.34      0.19       288\n",
      "     LargeZ-1       0.96      0.97      0.97     15751\n",
      "        other       0.26      0.42      0.32       981\n",
      "\n",
      "     accuracy                           0.81     80191\n",
      "    macro avg       0.53      0.62      0.56     80191\n",
      " weighted avg       0.83      0.81      0.82     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# 1. Standardize the features so that PCA and SMOTE work effectively.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Apply SMOTE on the training data to balance the minority classes.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 4. Apply PCA to reduce the feature space to 10 principal components.\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test)  # Use the same PCA transformation for test data.\n",
    "\n",
    "# 5. Train the Random Forest classifier on the PCA-transformed training data.\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "# 6. Make predictions and evaluate the model.\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_df = pd.read_csv(\"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_all.csv\")\n",
    "\n",
    "features = [\n",
    "    \"Loc\",\n",
    "    \"SITE\",\n",
    "    \"DOY\",\n",
    "    \"gdd2\",\n",
    "    \"WaterT\",\n",
    "    \"LAT0\",\n",
    "    \"LAT1\",\n",
    "    \"LON0\",\n",
    "    \"LON1\",\n",
    "    \"avgdepth\",\n",
    "    \"XANGLE\",\n",
    "    \"PRECIP\",\n",
    "    \"XWAVEHT\",\n",
    "    \"wind_direction\", \"wind_speed\",\n",
    "    \"CLOUD_PC\",\n",
    "    \"AvgDepth\",\n",
    "    \"Area..ABD.\",\n",
    "    \"Aspect.Ratio\",\n",
    "    \"Circularity\",\n",
    "    \"Compactness\",\n",
    "    \"Convexity\",\n",
    "    \"Elongation\",\n",
    "    \"Diameter..ABD.\",\n",
    "    \"Diameter..ESD.\",\n",
    "    \"Perimeter\",\n",
    "    \"Intensity\",\n",
    "    \"Sigma.Intensity\",\n",
    "    \"Roughness\",\n",
    "    \"Transparency\"\n",
    "]\n",
    "\n",
    "# One-hot encode the 'Loc' and 'SITE' columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Loc', 'SITE'], prefix=['Loc', 'SITE'])\n",
    "\n",
    "# Remove the original categorical columns from the features list\n",
    "if \"Loc\" in features:\n",
    "    features.remove(\"Loc\")\n",
    "if \"SITE\" in features:\n",
    "    features.remove(\"SITE\")\n",
    "\n",
    "# Add the new one-hot encoded columns for both 'Loc' and 'SITE'\n",
    "loc_dummy_cols = [col for col in merged_df.columns if col.startswith(\"Loc_\")]\n",
    "site_dummy_cols = [col for col in merged_df.columns if col.startswith(\"SITE_\")]\n",
    "features.extend(loc_dummy_cols)\n",
    "features.extend(site_dummy_cols)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA \n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
