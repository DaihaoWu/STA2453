{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (400955, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tifffile</th>\n",
       "      <th>csvfile</th>\n",
       "      <th>Year</th>\n",
       "      <th>SAM</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>repnum</th>\n",
       "      <th>Loc</th>\n",
       "      <th>SITE</th>\n",
       "      <th>DOY</th>\n",
       "      <th>...</th>\n",
       "      <th>Transparency</th>\n",
       "      <th>Image.File</th>\n",
       "      <th>Particle.ID</th>\n",
       "      <th>Image.Height</th>\n",
       "      <th>Image.Width</th>\n",
       "      <th>Image.X</th>\n",
       "      <th>Image.Y</th>\n",
       "      <th>Filename</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>1.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7494</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>6.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>10.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>12.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000023.tif</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>FISHI</td>\n",
       "      <td>STA</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>04072021_Huron_6_2mm_Rep4_AD_000001.tif</td>\n",
       "      <td>14.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>20210407_FISHI_006_2mm_Rep4_VC_data.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tifffile  \\\n",
       "0  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "1  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "2  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "3  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "4  04072021_Huron_6_2mm_Rep4_AD_000023.tif   \n",
       "\n",
       "                                   csvfile  Year  SAM  Month  Day  repnum  \\\n",
       "0  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "1  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "2  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "3  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "4  20210407_FISHI_006_2mm_Rep4_VC_data.csv  2021    6      4    7       4   \n",
       "\n",
       "     Loc SITE  DOY  ...  Transparency  \\\n",
       "0  FISHI  STA   97  ...        0.7957   \n",
       "1  FISHI  STA   97  ...        0.7494   \n",
       "2  FISHI  STA   97  ...        0.7751   \n",
       "3  FISHI  STA   97  ...        0.7866   \n",
       "4  FISHI  STA   97  ...        0.7755   \n",
       "\n",
       "                                Image.File  Particle.ID  Image.Height  \\\n",
       "0  04072021_Huron_6_2mm_Rep4_AD_000001.tif          1.0         295.0   \n",
       "1  04072021_Huron_6_2mm_Rep4_AD_000001.tif          6.0         263.0   \n",
       "2  04072021_Huron_6_2mm_Rep4_AD_000001.tif         10.0         166.0   \n",
       "3  04072021_Huron_6_2mm_Rep4_AD_000001.tif         12.0         253.0   \n",
       "4  04072021_Huron_6_2mm_Rep4_AD_000001.tif         14.0         173.0   \n",
       "\n",
       "   Image.Width  Image.X  Image.Y                                 Filename  \\\n",
       "0        276.0      0.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv   \n",
       "1         69.0    532.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv   \n",
       "2        106.0    862.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv   \n",
       "3         78.0    993.0      0.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv   \n",
       "4         79.0      0.0    297.0  20210407_FISHI_006_2mm_Rep4_VC_data.csv   \n",
       "\n",
       "   wind_direction  wind_speed  \n",
       "0              90           5  \n",
       "1              90           5  \n",
       "2              90           5  \n",
       "3              90           5  \n",
       "4              90           5  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset\n",
    "data_path = \"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_All_Clean.csv\"  # Update path if necessary\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Rename wind direction and wind speed columns\n",
    "# Ensure WIND is a string type\n",
    "df[\"WIND\"] = df[\"WIND\"].astype(str)\n",
    "\n",
    "# Split WIND into direction and speed\n",
    "wind_split = df[\"WIND\"].str.split(\"-\", expand=True)\n",
    "\n",
    "# Assign to new columns and convert to integers\n",
    "df[\"wind_direction\"] = pd.to_numeric(wind_split[0], errors=\"coerce\")\n",
    "df[\"wind_speed\"] = pd.to_numeric(wind_split[1], errors=\"coerce\")\n",
    "\n",
    "# Drop the original WIND column if no longer needed\n",
    "df.drop(columns=[\"WIND\"], inplace=True)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Selected Data and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8416904640171591\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.78      0.82      0.80      1390\n",
      "      Bubbles       0.94      0.86      0.90       945\n",
      "   Calanoid_1       0.80      0.85      0.83     16075\n",
      "   Chironomid       0.78      0.56      0.65       303\n",
      "   Chydoridae       0.00      0.00      0.00        18\n",
      "   CopepodSpp       0.51      0.34      0.41      1725\n",
      "   CountGT500       0.69      0.33      0.44      2575\n",
      "      Cyclo_2       0.59      0.44      0.50      3248\n",
      "  Cyclopoid_1       0.77      0.87      0.81     16973\n",
      "      Daphnia       0.50      0.11      0.18        83\n",
      "       Floc_1       0.93      0.94      0.93     19836\n",
      "Herpacticoida       0.31      0.04      0.07       288\n",
      "     LargeZ-1       0.95      0.98      0.96     15751\n",
      "        other       0.48      0.22      0.30       981\n",
      "\n",
      "     accuracy                           0.84     80191\n",
      "    macro avg       0.64      0.52      0.56     80191\n",
      " weighted avg       0.83      0.84      0.83     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is your target column\n",
    "# And your selected features are as defined\n",
    "features = [\"WaterT\", \"AvgDepth\", \"PRECIP\", \"gdd2\", \"DOY\", \"YPerchDen\", \n",
    "            \"BurbotDen\", \"OtherFishDen\", \"distshore\", \"Area..ABD.\", \"Aspect.Ratio\", \n",
    "            \"Circularity\", \"Perimeter\", \"Diameter..ABD.\", \"Diameter..ESD.\"]\n",
    "\n",
    "# Extract features and target\n",
    "X = df[features]\n",
    "y = df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 10 components\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with SMOTE RESAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805339751343667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.72      0.81      0.76      1390\n",
      "      Bubbles       0.87      0.91      0.89       945\n",
      "   Calanoid_1       0.84      0.78      0.81     16075\n",
      "   Chironomid       0.47      0.75      0.58       303\n",
      "   Chydoridae       0.07      0.28      0.11        18\n",
      "   CopepodSpp       0.30      0.45      0.36      1725\n",
      "   CountGT500       0.41      0.48      0.44      2575\n",
      "      Cyclo_2       0.43      0.56      0.49      3248\n",
      "  Cyclopoid_1       0.81      0.72      0.76     16973\n",
      "      Daphnia       0.13      0.30      0.18        83\n",
      "       Floc_1       0.95      0.90      0.93     19836\n",
      "Herpacticoida       0.14      0.34      0.19       288\n",
      "     LargeZ-1       0.96      0.97      0.97     15751\n",
      "        other       0.26      0.42      0.32       981\n",
      "\n",
      "     accuracy                           0.81     80191\n",
      "    macro avg       0.53      0.62      0.56     80191\n",
      " weighted avg       0.83      0.81      0.82     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# 1. Standardize the features so that PCA and SMOTE work effectively.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Apply SMOTE on the training data to balance the minority classes.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 4. Apply PCA to reduce the feature space to 10 principal components.\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test)  # Use the same PCA transformation for test data.\n",
    "\n",
    "# 5. Train the Random Forest classifier on the PCA-transformed training data.\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "# 6. Make predictions and evaluate the model.\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with more variables and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8778291828260029\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.84      0.84      0.84      1390\n",
      "      Bubbles       0.96      0.90      0.93       945\n",
      "   Calanoid_1       0.86      0.92      0.88     16075\n",
      "   Chironomid       0.87      0.71      0.78       303\n",
      "   Chydoridae       0.00      0.00      0.00        18\n",
      "   CopepodSpp       0.56      0.35      0.43      1725\n",
      "   CountGT500       0.77      0.43      0.55      2575\n",
      "      Cyclo_2       0.69      0.48      0.56      3248\n",
      "  Cyclopoid_1       0.81      0.91      0.86     16973\n",
      "      Daphnia       0.77      0.20      0.32        83\n",
      "       Floc_1       0.95      0.96      0.95     19836\n",
      "Herpacticoida       0.72      0.38      0.50       288\n",
      "     LargeZ-1       0.97      0.97      0.97     15751\n",
      "        other       0.60      0.29      0.39       981\n",
      "\n",
      "     accuracy                           0.88     80191\n",
      "    macro avg       0.74      0.60      0.64     80191\n",
      " weighted avg       0.87      0.88      0.87     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_df = pd.read_csv(\"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_all.csv\")\n",
    "\n",
    "merged_df = pd.read_csv(data_path)\n",
    "\n",
    "# Rename wind direction and wind speed columns\n",
    "# Ensure WIND is a string type\n",
    "merged_df[\"WIND\"] = merged_df[\"WIND\"].astype(str)\n",
    "\n",
    "# Split WIND into direction and speed\n",
    "wind_split = merged_df[\"WIND\"].str.split(\"-\", expand=True)\n",
    "\n",
    "# Assign to new columns and convert to integers\n",
    "merged_df[\"wind_direction\"] = pd.to_numeric(wind_split[0], errors=\"coerce\")\n",
    "merged_df[\"wind_speed\"] = pd.to_numeric(wind_split[1], errors=\"coerce\")\n",
    "\n",
    "# Drop the original WIND column if no longer needed\n",
    "merged_df.drop(columns=[\"WIND\"], inplace=True)\n",
    "\n",
    "features = [\n",
    "    \"Loc\",\n",
    "    \"SITE\",\n",
    "    \"DOY\",\n",
    "    \"gdd2\",\n",
    "    \"WaterT\",\n",
    "    \"LAT0\",\n",
    "    \"LAT1\",\n",
    "    \"LON0\",\n",
    "    \"LON1\",\n",
    "    \"avgdepth\",\n",
    "    \"XANGLE\",\n",
    "    \"PRECIP\",\n",
    "    \"XWAVEHT\",\n",
    "    \"wind_direction\", \"wind_speed\",\n",
    "    \"CLOUD_PC\",\n",
    "    \"AvgDepth\",\n",
    "    \"Area..ABD.\",\n",
    "    \"Aspect.Ratio\",\n",
    "    \"Circularity\",\n",
    "    \"Compactness\",\n",
    "    \"Convexity\",\n",
    "    \"Elongation\",\n",
    "    \"Diameter..ABD.\",\n",
    "    \"Diameter..ESD.\",\n",
    "    \"Perimeter\",\n",
    "    \"Intensity\",\n",
    "    \"Sigma.Intensity\",\n",
    "    \"Roughness\",\n",
    "    \"Transparency\"\n",
    "]\n",
    "\n",
    "# One-hot encode the 'Loc' and 'SITE' columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Loc', 'SITE'], prefix=['Loc', 'SITE'])\n",
    "\n",
    "# Remove the original categorical columns from the features list\n",
    "if \"Loc\" in features:\n",
    "    features.remove(\"Loc\")\n",
    "if \"SITE\" in features:\n",
    "    features.remove(\"SITE\")\n",
    "\n",
    "# Add the new one-hot encoded columns for both 'Loc' and 'SITE'\n",
    "loc_dummy_cols = [col for col in merged_df.columns if col.startswith(\"Loc_\")]\n",
    "site_dummy_cols = [col for col in merged_df.columns if col.startswith(\"SITE_\")]\n",
    "features.extend(loc_dummy_cols)\n",
    "features.extend(site_dummy_cols)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA \n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with SMOTE Resampling technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8653838959484231\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.82      0.87      0.84      1390\n",
      "      Bubbles       0.94      0.94      0.94       945\n",
      "   Calanoid_1       0.89      0.87      0.88     16075\n",
      "   Chironomid       0.71      0.79      0.74       303\n",
      "   Chydoridae       0.08      0.06      0.06        18\n",
      "   CopepodSpp       0.38      0.49      0.43      1725\n",
      "   CountGT500       0.55      0.53      0.54      2575\n",
      "      Cyclo_2       0.58      0.63      0.60      3248\n",
      "  Cyclopoid_1       0.85      0.82      0.84     16973\n",
      "      Daphnia       0.59      0.39      0.47        83\n",
      "       Floc_1       0.96      0.95      0.96     19836\n",
      "Herpacticoida       0.40      0.57      0.47       288\n",
      "     LargeZ-1       0.97      0.98      0.97     15751\n",
      "        other       0.39      0.50      0.43       981\n",
      "\n",
      "     accuracy                           0.87     80191\n",
      "    macro avg       0.65      0.67      0.66     80191\n",
      " weighted avg       0.87      0.87      0.87     80191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "merged_df = pd.read_csv(\"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_all.csv\")\n",
    "\n",
    "merged_df = pd.read_csv(data_path)\n",
    "\n",
    "# Rename wind direction and wind speed columns\n",
    "# Ensure WIND is a string type\n",
    "merged_df[\"WIND\"] = merged_df[\"WIND\"].astype(str)\n",
    "\n",
    "# Split WIND into direction and speed\n",
    "wind_split = merged_df[\"WIND\"].str.split(\"-\", expand=True)\n",
    "\n",
    "# Assign to new columns and convert to integers\n",
    "merged_df[\"wind_direction\"] = pd.to_numeric(wind_split[0], errors=\"coerce\")\n",
    "merged_df[\"wind_speed\"] = pd.to_numeric(wind_split[1], errors=\"coerce\")\n",
    "\n",
    "# Drop the original WIND column if no longer needed\n",
    "merged_df.drop(columns=[\"WIND\"], inplace=True)\n",
    "\n",
    "# Define the list of features\n",
    "features = [\n",
    "    \"Loc\",\n",
    "    \"SITE\",\n",
    "    \"DOY\",\n",
    "    \"gdd2\",\n",
    "    \"WaterT\",\n",
    "    \"LAT0\",\n",
    "    \"LAT1\",\n",
    "    \"LON0\",\n",
    "    \"LON1\",\n",
    "    \"avgdepth\",\n",
    "    \"XANGLE\",\n",
    "    \"PRECIP\",\n",
    "    \"XWAVEHT\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\",\n",
    "    \"CLOUD_PC\",\n",
    "    \"AvgDepth\",\n",
    "    \"Area..ABD.\",\n",
    "    \"Aspect.Ratio\",\n",
    "    \"Circularity\",\n",
    "    \"Compactness\",\n",
    "    \"Convexity\",\n",
    "    \"Elongation\",\n",
    "    \"Diameter..ABD.\",\n",
    "    \"Diameter..ESD.\",\n",
    "    \"Perimeter\",\n",
    "    \"Intensity\",\n",
    "    \"Sigma.Intensity\",\n",
    "    \"Roughness\",\n",
    "    \"Transparency\"\n",
    "]\n",
    "\n",
    "# One-hot encode the 'Loc' and 'SITE' columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Loc', 'SITE'], prefix=['Loc', 'SITE'])\n",
    "\n",
    "# Remove the original categorical columns from the features list\n",
    "if \"Loc\" in features:\n",
    "    features.remove(\"Loc\")\n",
    "if \"SITE\" in features:\n",
    "    features.remove(\"SITE\")\n",
    "\n",
    "# Add the new one-hot encoded columns for both 'Loc' and 'SITE'\n",
    "loc_dummy_cols = [col for col in merged_df.columns if col.startswith(\"Loc_\")]\n",
    "site_dummy_cols = [col for col in merged_df.columns if col.startswith(\"SITE_\")]\n",
    "features.extend(loc_dummy_cols)\n",
    "features.extend(site_dummy_cols)\n",
    "\n",
    "# Extract features and target\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Apply PCA to reduce dimensionality (here using all components; you can adjust n_components as needed)\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8641201764673033\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.78      0.88      0.82      1424\n",
      "      Bubbles       0.95      0.95      0.95      1155\n",
      "   Calanoid_1       0.90      0.86      0.88     20034\n",
      "   Chironomid       0.66      0.79      0.72       316\n",
      "   Chydoridae       0.22      0.18      0.20        22\n",
      "   CopepodSpp       0.36      0.52      0.43      1959\n",
      "   CountGT500       0.57      0.54      0.56      3017\n",
      "      Cyclo_2       0.59      0.68      0.63      3875\n",
      "  Cyclopoid_1       0.86      0.81      0.84     20007\n",
      "      Daphnia       0.41      0.54      0.47       129\n",
      "       Floc_1       0.97      0.95      0.96     19843\n",
      "Herpacticoida       0.38      0.59      0.46       340\n",
      "     LargeZ-1       0.98      0.99      0.99     16374\n",
      "        other       0.36      0.54      0.44      1040\n",
      "\n",
      "     accuracy                           0.86     89535\n",
      "    macro avg       0.64      0.70      0.67     89535\n",
      " weighted avg       0.88      0.86      0.87     89535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "merged_df = pd.read_csv(\"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_all.csv\")\n",
    "\n",
    "# Define the list of features\n",
    "features = [\n",
    "    \"Loc\",\n",
    "    \"SITE\",\n",
    "    \"DOY\",\n",
    "    \"gdd2\",\n",
    "    \"WaterT\",\n",
    "    \"LAT0\",\n",
    "    \"LAT1\",\n",
    "    \"LON0\",\n",
    "    \"LON1\",\n",
    "    \"avgdepth\",\n",
    "    \"XANGLE\",\n",
    "    \"PRECIP\",\n",
    "    \"XWAVEHT\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\",\n",
    "    \"CLOUD_PC\",\n",
    "    \"AvgDepth\",\n",
    "    \"Area..ABD.\",\n",
    "    \"Aspect.Ratio\",\n",
    "    \"Circularity\",\n",
    "    \"Compactness\",\n",
    "    \"Convexity\",\n",
    "    \"Elongation\",\n",
    "    \"Diameter..ABD.\",\n",
    "    \"Diameter..ESD.\",\n",
    "    \"Perimeter\",\n",
    "    \"Intensity\",\n",
    "    \"Sigma.Intensity\",\n",
    "    \"Roughness\",\n",
    "    \"Transparency\"\n",
    "]\n",
    "\n",
    "# One-hot encode the 'Loc' and 'SITE' columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Loc', 'SITE'], prefix=['Loc', 'SITE'])\n",
    "\n",
    "# Remove the original categorical columns from the features list\n",
    "if \"Loc\" in features:\n",
    "    features.remove(\"Loc\")\n",
    "if \"SITE\" in features:\n",
    "    features.remove(\"SITE\")\n",
    "\n",
    "# Add the new one-hot encoded columns for both 'Loc' and 'SITE'\n",
    "loc_dummy_cols = [col for col in merged_df.columns if col.startswith(\"Loc_\")]\n",
    "site_dummy_cols = [col for col in merged_df.columns if col.startswith(\"SITE_\")]\n",
    "features.extend(loc_dummy_cols)\n",
    "features.extend(site_dummy_cols)\n",
    "\n",
    "# Extract features and target\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train the Random Forest model on the SMOTE-resampled data\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without PCA and custom weights on variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {'Cyclopoid_1': 2.0, 'LargeZ-1': 1.0, 'Bosmina_1': 2.0, 'Floc_1': 1.0, 'Calanoid_1': 2.0, 'Cyclo_2': 1.0, 'CountGT500': 1.0, 'CopepodSpp': 1.0, 'other': 1.0, 'Chironomid': 2.0, 'Herpacticoida': 2.0, 'Bubbles': 1.0, 'Daphnia': 2.0, 'Chydoridae': 2.0}\n",
      "Accuracy: 0.8622549840844362\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Bosmina_1       0.79      0.88      0.83      1424\n",
      "      Bubbles       0.94      0.95      0.95      1155\n",
      "   Calanoid_1       0.90      0.86      0.88     20034\n",
      "   Chironomid       0.66      0.78      0.71       316\n",
      "   Chydoridae       0.25      0.18      0.21        22\n",
      "   CopepodSpp       0.36      0.52      0.42      1959\n",
      "   CountGT500       0.56      0.54      0.55      3017\n",
      "      Cyclo_2       0.57      0.68      0.62      3875\n",
      "  Cyclopoid_1       0.87      0.81      0.83     20007\n",
      "      Daphnia       0.41      0.54      0.47       129\n",
      "       Floc_1       0.97      0.95      0.96     19843\n",
      "Herpacticoida       0.37      0.60      0.46       340\n",
      "     LargeZ-1       0.98      0.99      0.99     16374\n",
      "        other       0.37      0.55      0.44      1040\n",
      "\n",
      "     accuracy                           0.86     89535\n",
      "    macro avg       0.64      0.70      0.67     89535\n",
      " weighted avg       0.87      0.86      0.87     89535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "merged_df = pd.read_csv(\"/Users/willwu/Documents/GitHub/Zooplankton/plankton_data/Merged_Master_all.csv\")\n",
    "\n",
    "# Define the list of features\n",
    "features = [\n",
    "    \"Loc\",\n",
    "    \"SITE\",\n",
    "    \"DOY\",\n",
    "    \"gdd2\",\n",
    "    \"WaterT\",\n",
    "    \"LAT0\",\n",
    "    \"LAT1\",\n",
    "    \"LON0\",\n",
    "    \"LON1\",\n",
    "    \"avgdepth\",\n",
    "    \"XANGLE\",\n",
    "    \"PRECIP\",\n",
    "    \"XWAVEHT\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\",\n",
    "    \"CLOUD_PC\",\n",
    "    \"AvgDepth\",\n",
    "    \"Area..ABD.\",\n",
    "    \"Aspect.Ratio\",\n",
    "    \"Circularity\",\n",
    "    \"Compactness\",\n",
    "    \"Convexity\",\n",
    "    \"Elongation\",\n",
    "    \"Diameter..ABD.\",\n",
    "    \"Diameter..ESD.\",\n",
    "    \"Perimeter\",\n",
    "    \"Intensity\",\n",
    "    \"Sigma.Intensity\",\n",
    "    \"Roughness\",\n",
    "    \"Transparency\"\n",
    "]\n",
    "\n",
    "# One-hot encode the 'Loc' and 'SITE' columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['Loc', 'SITE'], prefix=['Loc', 'SITE'])\n",
    "\n",
    "# Remove the original categorical columns from the features list\n",
    "if \"Loc\" in features:\n",
    "    features.remove(\"Loc\")\n",
    "if \"SITE\" in features:\n",
    "    features.remove(\"SITE\")\n",
    "\n",
    "# Add the new one-hot encoded columns for both 'Loc' and 'SITE'\n",
    "loc_dummy_cols = [col for col in merged_df.columns if col.startswith(\"Loc_\")]\n",
    "site_dummy_cols = [col for col in merged_df.columns if col.startswith(\"SITE_\")]\n",
    "features.extend(loc_dummy_cols)\n",
    "features.extend(site_dummy_cols)\n",
    "\n",
    "# Extract features and target\n",
    "X = merged_df[features]\n",
    "y = merged_df['Class']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the list of target classes to focus on\n",
    "target_classes = [\"Calanoid_1\", \"Cyclopoid_1\", \"Bosmina_1\", \n",
    "                  \"Herpacticoida\", \"Chydoridae\", \"Chironomid\", \"Daphnia\"]\n",
    "\n",
    "# Build a class weight dictionary: assign a higher weight (e.g., 2.0) to target classes, 1.0 to others\n",
    "class_weights = {}\n",
    "for c in y_train_smote.unique():\n",
    "    if c in target_classes:\n",
    "        class_weights[c] = 2.0  # Increase weight for target classes\n",
    "    else:\n",
    "        class_weights[c] = 1.0  # Keep default weight for non-target classes\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Create and train the Random Forest model with the custom class weights\n",
    "rf = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plankton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
